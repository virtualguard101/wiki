---
date: 2025-09-15 22:12:00
title: 梯度下降
permalink: 
publish: true
---

# 梯度下降

## 定义概述

在前面的学习中，我们知道训练模型过程中的一个目标是找到尽可能低的[成本函数](成本函数概述.md)值对应的模型函数参数。

在前面的线性回归模型中，参数数量与模型复杂度在机器学习领域中算是最简单的模型之一了，他们的成本函数也相对简单，比如房价预测模型的成本函数曲线/曲面，是一个凸型成本曲面，==只有一个极小值，并且就对应该成本函数的最小值==。但对于相对复杂的模型函数，就有更加复杂的成本函数，而且就算是最简单的线性回归模型，也不是只有均方误差函数一种成本函数可以用......

然后，你可能就会看到类似下图的成本函数:

![复杂复杂成本函数](assets/gradient/gradient_descent_0.jpg)

这是一个在训练神经网络模型时可能会用到的成本函数。首先，它包含不止一个极小值，其次，我们也不知道哪个极小值才是最小成本值.....

显然，我们急切地需要一个高效且通用的方法来找到最小的 $J(\theta)$，为此，**梯度下降算法（*Gradient Descent Algorithm*）**应运而生。

!!! example "梯度下降的通俗解释"
    打个比方，假设我们站在一座山的山顶，梯度下降算法要做的，就是帮助我们找到一条能够快速通向山谷的路径。
    
    具体要怎么实现呢？试想如果我们要朝某个方向迈出微小的一步，且希望以最快速度下山到达这些山谷之一，我们该选择哪个方向迈出这一步？
    
    抛开其他问题不谈，我们显然应该选择一个坡度最陡峭的方位，==在数学上，这个方位上函数的增长方向就是一个**向量**，通过对多元函数的各个变量求偏导获取，被称为**梯度（*Gradient*）**==：

    $$
    \nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n})
    $$

    !!! warning "梯度方向与负梯度方向"
        这里有一个容易混淆的地方：数学上，梯度方向是指函数值**增长**最快的方向，即“上山方向”；而我们下山的方向事实是上一个**负梯度方向**，即函数值**减少**最快的方向，与对应的梯度方向相反。

    继续刚才的问题，在迈出第一步后，我们下一步要做的就是重复刚刚的动作，环顾四周，继续寻找“最快的下山方向”，以此类推，直到你发现自己位于某个山谷的底部，在数学上就是一个极小值点。

## 实现算法

采用重复执行的方式不断更新参数的值，直至 $J(\theta)$ 收敛至某个极小值:

$$
w = w - \alpha\frac{\partial J(w, b)}{\partial w}
$$

$$
b = b - \alpha\frac{\partial J(w, b)}{\partial b}
$$

!!! warning
    - 注意这里的 $=$ 是**赋值符号**，而不是**相等断言**

    - 注意 $w$ 和 $b$ 的更新是**同时进行**的，也就是说下面的这种执行顺序是错误的:

        !!! failure
            1.

            $$
            w_{tmp} = w - \alpha\frac{\partial J(w, b)}{\partial w}
            $$

            ⚠️2.

            $$
            w = w_{tmp}
            $$

            3.

            $$ 
            b_{tmp} = b - \alpha\frac{\partial J(w, b)}{\partial b}
            $$

            4.

            $$
            b = b_{tmp}
            $$

        因为这样在执行第三步时，$b_{tmp}$ 的值需要用到 $w$ 来计算 $J(w, b)$ 的值，而这时的 $w$ 已经被更新了

这里 $\alpha$ 是**学习率（*Learning Rate*）**，简单理解就是用来控制“下山过程中的某一步**一次性走多远**”的指标，后面会详细介绍。

## 学习率

**学习率（*Learning Rate*）**是机器学习优化算法中的关键超参数，控制模型参数更新的步长大小。它决定了模型在每次迭代中沿着梯度方向移动的距离，直接影响模型收敛速度和最终性能。

其范围通常设定在 $0.001$ 到 $0.1$ 之间，具体值需要根据问题调整

### 设定问题

- **学习率过大**

    - 模型可能跳过最优解

    - 训练过程不稳定，成本函数值震荡

    - 甚至可能导致梯度爆炸

- **学习率过小**

    - 训练速度极慢

    - 可能陷入局部最优

    - 资源浪费严重

### 设定策略

- 固定学习率

    即便学习率固定，它也是可以将你引导至成本函数的某个极小值。极小值处的导数值为 $0$，当成本函数收敛到这里后，学习率作为一个系数也就不会在起到作用了。但是，

    - ==其也有可能只是收敛到**局部最优**而非全局最优==

    - 在某些情况下可能不收敛（学习率过大时）

    - 在某些收敛速度可能很慢

- 学习率衰减

    $$
    \alpha_t = \alpha_0 \times \gamma^t
    $$

    $$
    \alpha_t = \alpha_0 \times e^{-\lambda t}
    $$

    这是由于越靠近成本函数极小值，其导数变化通常较小，因此这样可以避免在最优解附近震荡，同时提高收敛精度。

- 自适应学习率

    根据梯度变化自动调整（如[Adam](https://ceur-ws.org/Vol-3742/paper17.pdf)、[RMSprop](https://www.datacamp.com/tutorial/rmsprop-optimizer-tutorial)）

## 一元线性回归模型梯度下降公式的简单推导

需要用到微积分中的偏导数:

$$
\begin{aligned}
    \frac{\partial J(w, b)}{\partial w} & = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)}) \\
    & = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i=1}^{m}(wx^{i} + b - y^{(i)})^2 \\
    & = \frac{1}{\not {2}m} \sum_{i=1}^{m}(wx^{i} + b - y^{(i)}) \not {2} x^{(i)} \\
    & = \frac{1}{m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{i})x^{(i)}
\end{aligned}
$$

$b$ 同理，将求导对象换成 $b$ 即可:

$$
\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{i})
$$

然后将上述的两个推导结果代入前面的实现算法即可。
