# 梯度下降

## 定义概述

在前面的学习中，我们知道训练模型的目标是找到尽可能低的成本函数值对应的模型函数参数。

在前面的线性回归模型中，参数数量与模型复杂度在机器学习领域中算是最简单的模型之一了，他们的成本函数也相对简单，比如房价预测模型的成本函数曲线/曲面，是一个凸型成本曲面，==只有一个极小值，并且就对应该成本函数的最小值==。但对于相对复杂的模型函数，就有更加复杂的成本函数，而且就算是最简单的线性回归模型，也不是只有均方误差函数一种成本函数可以用......

然后，你可能就会看到类似下图的成本函数:

![复杂复杂成本函数](gradient_descent_0.jpg)

这是一个在训练神经网络模型时可能会用到的成本函数。首先，它包含不止一个极小值，其次，我们也不知道哪个极小值才是最小成本值.....

显然，我们急切地需要一个高效且通用的方法来找到最小的 $J(\theta)$，为此，**梯度下降算法（*Gradient Descent Algorithm*）**应运而生。

!!! example "梯度下降的通俗解释"
    打个比方，假设我们站在一座山的山顶，梯度下降算法要做的，就是帮助我们找到一条能够快速通向山谷的路径。
    
    具体要怎么实现呢？试想如果我们要朝某个方向迈出微小的一步，且希望以最快速度下山到达这些山谷之一，我们该选择哪个方向迈出这一步？
    
    抛开其他问题不谈，我们显然应该选择一个坡度最陡峭的方位，==在数学上，这个方位上函数的增长方向就是一个**向量**，通过对多元函数的各个变量求偏导获取，被称为**梯度（*Gradient*）**==：

    $$
    \nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n})
    $$

    !!! warning "梯度方向与负梯度方向"
        这里有一个容易混淆的地方：数学上，梯度方向是指函数值**增长**最快的方向，即“上山方向”；而我们下山的方向事实是上一个**负梯度方向**，即函数值**减少**最快的方向，与对应的梯度方向相反。

    继续刚才的问题，在迈出第一步后，我们下一步要做的就是重复刚刚的动作，环顾四周，继续寻找“最快的下山方向”，以此类推，直到你发现自己位于某个山谷的底部，在数学上就是一个极小值点。

## 实现算法

采用重复执行的方式不断更新参数的值，直至 $J(\theta)$ 收敛至某个极小值:

$$
w = w - \alpha\frac{\partial J(w, b)}{\partial w}
$$

$$
b = b - \alpha\frac{\partial J(w, b)}{\partial b}
$$

!!! warning
    - 注意这里的 $=$ 是**赋值符号**，而不是**相等断言**

    - 注意 $w$ 和 $b$ 的更新是**同时进行**的，也就是说下面的这种执行顺序是错误的:

        !!! failure
            1.

            $$
            w_{tmp} = w - \alpha\frac{\partial J(w, b)}{\partial w}
            $$

            ⚠️2.

            $$
            w = w_{tmp}
            $$

            3.

            $$ 
            b_{tmp} = b - \alpha\frac{\partial J(w, b)}{\partial b}
            $$

            4.

            $$
            b = b_{tmp}
            $$

        因为这样在执行第三步时，$b_{tmp}$ 的值需要用到 $w$ 来计算 $J(w, b)$ 的值，而这时的 $w$ 已经被更新了

这里 $\alpha$ 是**学习率（*Learning Rate*）**，简单理解就是用来控制“下山过程中的某一步**一次性走多远**”的指标，后面会详细介绍。

## 学习率

**学习率（*Learning Rate*）**是机器学习优化算法中的关键超参数，控制模型参数更新的步长大小。它决定了模型在每次迭代中沿着梯度方向移动的距离，直接影响模型收敛速度和最终性能。

其范围通常设定在 $0.001$ 到 $0.1$ 之间，具体值需要根据问题调整

### 设定问题

- **学习率过大**

    - 模型可能跳过最优解

    - 训练过程不稳定，成本函数值震荡

    - 甚至可能导致梯度爆炸

- **学习率过小**

    - 训练速度极慢

    - 可能陷入局部最优

    - 资源浪费严重

### 设定策略

- 固定学习率

    即便学习率固定，它也是可以将你引导至成本函数的某个极小值。极小值处的导数值为 $0$，当成本函数收敛到这里后，学习率作为一个系数也就不会在起到作用了。但是，

    - ==其也有可能只是收敛到**局部最优**而非全局最优==

    - 在某些情况下可能不收敛（学习率过大时）

    - 在某些收敛速度可能很慢

- 学习率衰减

    $$
    \alpha_t = \alpha_0 \times \gamma^t
    $$

    $$
    \alpha_t = \alpha_0 \times e^{-\lambda t}
    $$

    这是由于越靠近成本函数极小值，其导数变化通常较小，因此这样可以避免在最优解附近震荡，同时提高收敛精度。

- 自适应学习率

    根据梯度变化自动调整（如[Adam](https://ceur-ws.org/Vol-3742/paper17.pdf)、[RMSprop](https://www.datacamp.com/tutorial/rmsprop-optimizer-tutorial)）

## 一元线性回归模型梯度下降公式的简单推导

需要用到微积分中的偏导数:

$$
\begin{aligned}
    \frac{\partial J(w, b)}{\partial w} & = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)}) \\
    & = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i=1}^{m}(wx^{i} + b - y^{(i)})^2 \\
    & = \frac{1}{\not {2}m} \sum_{i=1}^{m}(wx^{i} + b - y^{(i)}) \not {2} x^{(i)} \\
    & = \frac{1}{m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{i})x^{(i)}
\end{aligned}
$$

$b$ 同理，将求导对象换成 $b$ 即可:

$$
\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{i})
$$

然后将上述的两个推导结果代入前面的实现算法即可。
